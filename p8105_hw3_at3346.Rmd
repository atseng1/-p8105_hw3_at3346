---
title: "p8105_hw3_at3346"
author: "Ashley Tseng"
date: "10/14/2019"
output: github_document
---

```{r setup, include=FALSE}
library(tidyverse)
library(p8105.datasets)
library(ggplot2)
library(viridis)
library(readxl)
library(tools)

knitr::opts_chunk$set(
	echo = TRUE,
	warning = FALSE,
	fig.width = 8, 
  fig.height = 6,
  out.width = "90%"
)
options(
  ggplot2.continuous.colour = "viridis",
  ggplot2.continuous.fill = "viridis"
)
scale_colour_discrete = scale_colour_viridis_d
scale_fill_discrete = scale_fill_viridis_d
theme_set(theme_minimal() + theme(legend.position = "bottom"))
```


## Problem 1

Load data:
```{r problem 1.1}
data("instacart") 
force(instacart)

instacart %>% 
  filter(order_hour_of_day == "23") %>% 
  count (product_name, name = "n_latenight")

n_aisle = instacart %>% 
  count(aisle)
```
The `instacart` dataset contains `r nrow(instacart)` observations (representing products from orders) of 131209 unique users and `r ncol(instacart)` variables. There is a single order per user in this dataset. Key variables in the `instacart` dataset include `reordered`, which describes if this product has been ordered by this user in the past,`order_dow`, which describes the day of the week on which the order was placed, and
`order_hour_of_day`, which describes the hour of the day on which the order was placed. For example, `user_id` 18 ordered 2 items (Organic Strawberries and Small Hass Avocado) from the fresh fruits aisle, both of which were reordered items. Another example is that the most ordered items at 11:00 pm from Instacart are "Banana," "Bag of Organic Bananas," "Organic Strawberries," and "Organic Baby Spinach," which is surprising given that I would expect more unhealthy food to be ordered late at night.

There are `r nrow(n_aisle)` total aisles included in the `instacart` dataset. The aisles that the most items are ordered from are "fresh vegetables" (150609 orders) and "fresh fruit" (150473 orders).


Make a table showing the three most popular items in each of the aisles “baking ingredients”, “dog food care”, and “packaged vegetables fruits”:
```{r prob1.plot1}
n_ordered = n_aisle %>% 
  filter(n > 10000) %>% 
  ggplot(aes(x = aisle, y = n)) +
  geom_bar(stat = "identity", fill = "sea green") +
  coord_flip() +
  labs(
    title = "Number of Items Ordered in Each Aisle",
    x = "Aisle Name",
    y = "Number of Items Ordered") +
  theme(
    legend.position = "none",
    plot.title = element_text(hjust = 0.5),
    text = element_text(size = 7))

n_ordered
```


Make a table showing the mean hour of the day at which Pink Lady Apples and Coffee Ice Cream are ordered on each day of the week:
```{r prob1.table1}
table_popitems = instacart %>% 
  filter(aisle == "baking ingredients" | aisle == "dog food care" | aisle == "packaged vegetables fruits") %>%
  mutate(aisle = toTitleCase(aisle)) %>% 
  group_by(aisle) %>%
  count(product_name, name = "n_prod") %>% 
  filter(dense_rank(desc(n_prod)) < 4) %>% 
  arrange(desc(n_prod)) %>% 
  rename (
  "Aisle" = aisle,
  "Product Name" = product_name,
  "Number of Times Ordered" = n_prod) 

table_popitems %>% 
  knitr::kable()
```


Make a table showing the mean hour of the day at which Pink Lady Apples and Coffee Ice Cream are ordered on each day of the week:
```{r prob1.table2}
table_meanhr = instacart %>% 
  filter(product_name == "Pink Lady Apples" | product_name == "Coffee Ice Cream") %>% 
  mutate(
    order_dow = recode(order_dow, "0" = "Sunday", "1" = "Monday", "2" = "Tuesday", 
                       "3" = "Wednesday", "4" = "Thursday", "5" = "Friday", "6" = "Saturday"),
    order_dow = ordered(order_dow, c("Sunday", "Monday", "Tuesday", "Wednesday", "Thursday", "Friday", "Saturday"))                   
    ) %>% 
  group_by(product_name, order_dow) %>% 
  summarize(mean_hrordered = round(mean(order_hour_of_day), digits = 2)) %>% 
  separate(mean_hrordered, into = c("hour", "minute"), sep = "[.]") %>% 
  mutate(
    hour = as.numeric(hour),
    minute = as.numeric(minute),
    hour = case_when(
      hour > 12 ~ (hour - 12),
      hour <= 12 ~ hour),
    minute = round((minute/100)*60),
    minute = str_pad(minute, 2, pad = "0"),
    time = paste(hour, minute, sep = ":"),
    time = case_when(
      grepl("11", time) ~ (paste(time, 'am')),
      time < 11 ~ (paste(time, 'pm')),
      time > 12 ~ (paste(time, 'pm')))
    ) %>% 
    select(-c("hour", "minute")) %>% 
  pivot_wider(
    names_from = order_dow,
    values_from = time
  ) %>% 
  rename ("Product Name" = product_name) 

table_meanhr %>% 
  knitr::kable()
```



## Problem 2

Data cleaning:
```{r prob2.1}
data("brfss_smart2010") 
force(brfss_smart2010)

brfss_smart2010 = brfss_smart2010 %>% 
  janitor::clean_names() %>% 
  filter(
    topic == "Overall Health",
    response == "Excellent" | response == "Very good" | response == "Good" | response == "Fair" | response == "Poor") %>% 
  mutate(response = ordered(response, c("Poor", "Fair", "Good", "Very good", "Excellent")))
```


In 2002, which states were observed at 7 or more locations? What about in 2010?
```{r prob2.2}
seven_2002 = brfss_smart2010 %>% 
  filter(year == "2002") %>% 
  separate(locationdesc, into = c("state", "county"), sep = 4) %>% 
  select(-c("state")) %>% 
  rename("state" = locationabbr) %>% 
  group_by(state) %>%
  summarize(n_loc = n_distinct(county)) %>% 
  filter(n_loc > 6) %>% 
  arrange(desc(n_loc)) 


seven_2010 = brfss_smart2010 %>% 
  filter(year == "2010") %>% 
  separate(locationdesc, into = c("state", "county"), sep = 4) %>% 
  select(-c("state")) %>% 
  rename("state" = locationabbr) %>% 
  group_by(state) %>%
  summarize(n_loc = n_distinct(county)) %>% 
  filter(n_loc > 6) %>% 
  arrange(desc(n_loc)) 
```
In 2002, CT, FL, MA, NC, NJ, and PA were observed at 7 or more locations. 
In 2010, CA, CO, FL, MA, MD, NC, NE, NJ, NY, OH, PA, SC, TX, and WA were observed at 7 or more locations. 


Make a “spaghetti” plot of this average value over time within a state:
```{r prob2.3.plot1}
spaghetti = brfss_smart2010 %>% 
  filter(response == "Excellent") %>% 
  separate(locationdesc, into = c("state", "county"), sep = 4) %>% 
  select(-c("state")) %>% 
  rename("state" = locationabbr) %>% 
  group_by(year, state) %>% 
  mutate(avg_dv = mean(data_value)) %>% 
  select(year, state, avg_dv) %>%
  ggplot(aes(x = year, y = avg_dv, color = state)) + 
  geom_line() +
  labs(
    title = "Average Percentage of Respondents Rating General Health As 'Excellent' Over Time Across Locations Within Each State",
    x = "Year",
    y = "Average Percentage of Respondents Rating General Health As 'Excellent'",
    color = "State") +
  theme(plot.title = element_text(hjust = 0.8),
        legend.position = "right")

spaghetti
```


Make a two-panel plot showing, for the years 2006, and 2010, distribution of data_value for responses (“Poor” to “Excellent”) among locations in NY State:
```{r prob2.plot2}
two_panel_scat = brfss_smart2010 %>% 
  janitor::clean_names() %>% 
  filter(
    response == "Excellent" | response == "Very good" | response == "Good" | response == "Fair" | response == "Poor",
    year == "2006" | year == "2010",
    locationabbr == "NY") %>% 
  mutate(response = ordered(response, c("Poor", "Fair", "Good", "Very good", "Excellent"))) %>% 
  separate(locationdesc, into = c("state", "county"), sep="-") %>% 
  select(-c("locationabbr")) %>% 
  group_by(year, county, response) %>% 
  mutate(avg_dv = mean(data_value)) %>% 
  select(year, state, county, response, avg_dv) %>% 
  ggplot(aes(x = county, y = avg_dv, color = response, group = response)) + 
  geom_point() +
  geom_smooth(se = FALSE) +
  facet_grid(~year) +
  labs(
    title = "Distribution of Data Values For Responses Among Locations in NY State",
    x = "County in NY",
    y = "Average Data Value",
    color = "Response") +
  theme(plot.title = element_text(hjust = 0.5),
        axis.text.x = element_text(angle = 90, hjust = 1),
        panel.spacing = unit(1, "lines"),
        panel.border = element_rect(color = "black", fill = NA, size = 0.75))

two_panel_scat
```



## Problem 3

Load, tidy, and wrangle data:
```{r problem3.1}
accel_data = 
  read_csv("./data/accel_data.csv", col_names = TRUE) %>%
  janitor::clean_names() %>%
  mutate(
    weekend = recode(day, "Saturday" = 1, "Sunday" = 1, "Monday" = 0, "Tuesday" = 0, "Wednesday" = 0, "Thursday" = 0, "Friday" = 0),
    weekday = recode(day, "Saturday" = 0, "Sunday" = 0, "Monday" = 1, "Tuesday" = 1, "Wednesday" = 1, "Thursday" = 1, "Friday" = 1),
    day = ordered(day, c("Saturday", "Sunday", "Monday", "Tuesday", "Wednesday", "Thursday", "Friday")),
    week = ordered(week, c("1", "2", "3", "4", "5"))) %>% 
  select(week, day_id, day, weekend, weekday, everything()) %>% 
  pivot_longer(
    cols = starts_with("activity_"),
    names_to = "activity_minute_num",
    names_prefix = "activity_",
    values_to = "activity_counts") %>% 
  mutate(activity_minute_num = as.numeric(activity_minute_num))
```
The `accel_data` dataset contains `r nrow(accel_data)` observations and `r ncol(accel_data)` variables. The `r ncol(accel_data)` variables in the dataset are `week`, `day_id`, `day`, `weekend`, `weekday`, `activity_minute_num`, and `activity_counts`.


Aggregate accross minutes to create a total activity variable for each day, and create a table showing these totals:
```{r prob3.table1}
tot_act = accel_data %>% 
  group_by(week, day_id, day) %>% 
  summarize (total_min = sum(activity_counts)) %>%
  select(week, day_id, day, total_min)

knitr::kable(tot_act)

trends_wk = tot_act %>% 
  ggplot(aes(x = day, y = total_min, color = week)) + 
  geom_point() +
  geom_smooth(se = FALSE) +
  facet_grid(~week) +
  labs(
    title = "Total daily activity over 5 weeks",
    x = "Day of the week",
    y = "Total activity per day (min)") +
  theme(axis.text.x = element_text(angle = 90, hjust = 1))

trends_day = tot_act %>% 
  ggplot(aes(x = week, y = total_min, color = day)) + 
  geom_point() +
  geom_smooth(se = FALSE) +
  facet_grid(~day) +
  labs(
    title = "Total daily activity over 5 weeks",
    x = "Week",
    y = "Total activity per day (min)") 
```
Overall, there doesn't seem to be any trends apparent. If we want to examine for trends by week, for weeks 1 and 2, the total activity per day (in minutes) is lowest on Mondays, then gradually increases throughout the week into the weekend. In week 3, the total activity per day (in minutes) remains relatively consistent on all days of the week except for Monday, which has the highest total daily activity. For weeks 4 and 5, there is the lowest total daily activity on the weekends.


Make a single-panel plot that shows the 24-hour activity time courses for each day and use color to indicate day of the week:
```{r prob3.plot1}
avg_accel_data = accel_data %>% 
  group_by(week, day_id, day) %>% 
  summarize(avg_ct = mean(activity_counts)) %>% 
  ggplot(aes(x = day_id, y = avg_ct, color = day)) + 
  geom_line() +
  labs(
    title = "24-hour activity time courses for each day of the week over a 5-week period",
    x = "Minute Number",
    y = "Total activity count",
    color = "Day") +
  theme(plot.title = element_text(hjust = 0.5),
        legend.position = "right")

avg_accel_data
```

On average, the 24-hour time activity increases throughout the day on all days except for Sundays. Fridays have the greatest total activity count towards the end of the day.

